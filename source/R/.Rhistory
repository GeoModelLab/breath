rangeMax = 10.0
),
list(
name = "HU",
file = "hu_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Relative Humidity",
unit = "%",
rangeMin = 0.0,
rangeMax = 100.0
)
)
# Process each attribute
attributes_info <- list()
for (attr_cfg in attributes_config) {
nc_path <- file.path(config$dataFolder, attr_cfg$file)
if (!file.exists(nc_path)) {
warning(sprintf("File not found: %s - skipping", nc_path))
next
}
attr_info <- process_attribute(
nc_path = nc_path,
config = config,
attr_name = attr_cfg$name,
attr_desc = attr_cfg$description,
attr_unit = attr_cfg$unit,
range_min = attr_cfg$rangeMin,
range_max = attr_cfg$rangeMax
)
attributes_info[[attr_cfg$name]] <- attr_info
}
# Write lookup table (use first attribute's coordinate info)
first_attr <- attributes_info[[1]]
num_valid_coords <- write_lookup_table(config, first_attr$lonFiltered, first_attr$latFiltered,
first_attr$lonIndices, first_attr$latIndices)
# Calculate processing time
end_time <- Sys.time()
processing_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
# Write specification XML
write_specification_xml(config, attributes_info, num_valid_coords, processing_time)
cat("\n============================================================\n")
cat(sprintf("Processing complete! Total time: %.2f seconds\n", processing_time))
cat("============================================================\n")
}
# Run main
main()
# ============================================================================
# E-OBS Chunk Exporter
#
# Reads E-OBS NetCDF files and exports temporal chunks as binary files
# Configuration via config.xml
# Outputs: raw float32 binary files + specification.xml
# ============================================================================
library(ncdf4)
library(XML)
rm(list = ls())
# Set working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# ============================================================================
# 1. READ CONFIGURATION FROM XML
# ============================================================================
read_config <- function(config_path = "config.xml") {
if (!file.exists(config_path)) {
stop(sprintf("Config file not found: %s", config_path))
}
doc <- xmlParse(config_path)
root <- xmlRoot(doc)
# Use xmlGetText for more robust XML parsing
get_xml_value <- function(node_name) {
node <- root[[node_name]]
if (is.null(node)) {
stop(sprintf("Missing required XML element: %s", node_name))
}
xmlValue(node)
}
config <- list(
startYear = as.integer(get_xml_value("StartYear")),
endYear = as.integer(get_xml_value("EndYear")),
chunkDays = as.integer(get_xml_value("ChunkDays")),
minLongitude = as.numeric(get_xml_value("MinLongitude")),
maxLongitude = as.numeric(get_xml_value("MaxLongitude")),
minLatitude = as.numeric(get_xml_value("MinLatitude")),
maxLatitude = as.numeric(get_xml_value("MaxLatitude")),
dataFolder = get_xml_value("DataFolder")
)
free(doc)
return(config)
}
# ============================================================================
# 2. HELPER FUNCTIONS
# ============================================================================
# Null coalescing operator
`%||%` <- function(a, b) if (!is.null(a)) a else b
# Get time values and units from NetCDF
get_time_vals_units <- function(nc) {
vals <- tryCatch(nc$dim$time$vals, error = function(e) NULL)
units <- tryCatch(nc$dim$time$units, error = function(e) NULL)
if (is.null(vals) || length(vals) == 0) {
vals <- tryCatch(ncvar_get(nc, "time"), error = function(e) NULL)
if (!is.null(vals) && length(vals) > 0) {
units <- units %||% tryCatch(ncatt_get(nc, "time", "units")$value, error = function(e) NULL)
}
}
if (is.null(vals) || length(vals) == 0) {
stop("Cannot extract time values from NetCDF file")
}
list(vals = vals, units = units)
}
# Convert NetCDF time to R Date
nc_time_to_Date <- function(nc) {
tu <- get_time_vals_units(nc)
vals <- tu$vals
units <- tolower(tu$units)
m <- regexec("^(seconds|minutes|hours|days) since\\s*([0-9]{4}-[0-9]{2}-[0-9]{2})(?:[ T]([0-9]{2}:[0-9]{2}:[0-9]{2}))?", units)
mm <- regmatches(units, m)[[1]]
if (length(mm) < 3) {
stop(sprintf("Cannot parse time units: %s", units))
}
unit <- mm[2]
origin_date <- mm[3]
origin_time <- ifelse(length(mm) >= 4 && !is.na(mm[4]), mm[4], "00:00:00")
origin <- as.POSIXct(paste(origin_date, origin_time), tz = "UTC")
mult <- switch(unit,
seconds = 1,
minutes = 60,
hours = 3600,
days = 86400)
as.Date(origin + vals * mult, tz = "UTC")
}
# Read spatial block from NetCDF
read_spatial_block <- function(nc, varname, lonname, latname, lon_indices, lat_indices, time_start, time_count) {
vinfo <- nc$var[[varname]]
dnames <- sapply(vinfo$dim, `[[`, "name")
dlen <- sapply(vinfo$dim, `[[`, "len")
names(dlen) <- dnames
start <- setNames(rep(1L, length(dnames)), dnames)
count <- dlen
start[lonname] <- min(lon_indices)
count[lonname] <- length(lon_indices)
start[latname] <- min(lat_indices)
count[latname] <- length(lat_indices)
start["time"] <- time_start
count["time"] <- time_count
arr <- ncvar_get(nc, varid = varname, start = start[dnames], count = count[dnames], raw_datavals = TRUE)
# Apply scale and offset if present
sf <- tryCatch(ncatt_get(nc, varname, "scale_factor")$value, error = function(e) NA)
of <- tryCatch(ncatt_get(nc, varname, "add_offset")$value, error = function(e) NA)
mv <- tryCatch(ncatt_get(nc, varname, "_FillValue")$value, error = function(e) NA)
if (!is.na(sf) && !is.na(of)) {
arr <- of + sf * arr
}
if (!is.na(mv)) {
arr[arr == mv] <- NA_real_
}
# Return as [lon, lat, time]
aperm(arr, perm = c(match(lonname, dnames), match(latname, dnames), match("time", dnames)))
}
# ============================================================================
# 3. MAIN PROCESSING FUNCTION
# ============================================================================
process_attribute <- function(nc_path, config, attr_name, attr_desc, attr_unit, range_min, range_max) {
cat(sprintf("\n=== Processing %s (%s) ===\n", attr_name, nc_path))
# Open NetCDF
nc <- nc_open(nc_path, suppress_dimvals = TRUE)
on.exit(nc_close(nc), add = TRUE)
# Get variable name (assume first variable)
varname <- names(nc$var)[1]
# Get dimension names
dnames <- sapply(nc$var[[varname]]$dim, `[[`, "name")
lonname <- intersect(c("lon", "longitude"), dnames)[1]
latname <- intersect(c("lat", "latitude"), dnames)[1]
if (is.na(lonname) || is.na(latname)) {
stop("Cannot identify lon/lat dimensions")
}
# Get full coordinate arrays
lon_full <- ncvar_get(nc, lonname)
lat_full <- ncvar_get(nc, latname)
# Filter coordinates by bounds
lon_mask <- lon_full >= config$minLongitude & lon_full <= config$maxLongitude
lat_mask <- lat_full >= config$minLatitude & lat_full <= config$maxLatitude
lon_indices <- which(lon_mask)
lat_indices <- which(lat_mask)
lon_filtered <- lon_full[lon_indices]
lat_filtered <- lat_full[lat_indices]
num_coords <- length(lon_indices) * length(lat_indices)
cat(sprintf("  Coordinates: %d x %d = %d (filtered from %d x %d)\n",
length(lon_indices), length(lat_indices), num_coords,
length(lon_full), length(lat_full)))
# Get dates
dates <- nc_time_to_Date(nc)
# Filter by year range
start_date <- as.Date(sprintf("%d-01-01", config$startYear))
end_date <- as.Date(sprintf("%d-12-31", config$endYear))
date_mask <- dates >= start_date & dates <= end_date
time_indices <- which(date_mask)
dates_filtered <- dates[time_indices]
total_days <- length(dates_filtered)
cat(sprintf("  Time range: %s to %s (%d days)\n",
format(min(dates_filtered)), format(max(dates_filtered)), total_days))
# Calculate chunks
chunk_days <- config$chunkDays
num_chunks <- ceiling(total_days / chunk_days)
cat(sprintf("  Creating %d chunks of %d days each\n", num_chunks, chunk_days))
# Create output directory
attr_dir <- file.path(config$dataFolder, attr_name)
dir.create(attr_dir, showWarnings = FALSE, recursive = TRUE)
# Process each chunk
chunk_info <- list()
for (chunk_idx in 1:num_chunks) {
# Calculate chunk time range
chunk_start_day <- (chunk_idx - 1) * chunk_days + 1
chunk_end_day <- min(chunk_idx * chunk_days, total_days)
chunk_num_days <- chunk_end_day - chunk_start_day + 1
time_start_global <- time_indices[chunk_start_day]
chunk_start_date <- dates_filtered[chunk_start_day]
chunk_end_date <- dates_filtered[chunk_end_day]
chunk_start_year <- as.integer(format(chunk_start_date, "%Y"))
chunk_end_year <- as.integer(format(chunk_end_date, "%Y"))
chunk_day_of_year <- as.integer(format(chunk_start_date, "%j"))
# Filename with absolute day indices and year range
chunk_filename <- sprintf("chunk_%04d-%04d_%d-%d.bin",
chunk_start_day,      # Absolute start day (e.g., 0001)
chunk_end_day,        # Absolute end day (e.g., 0730)
chunk_start_year,     # Year of first entry
chunk_end_year)       # Year of last entry
chunk_path <- file.path(attr_dir, chunk_filename)
cat(sprintf("  Chunk %d/%d: %s (%d days, %s to %s)\n",
chunk_idx, num_chunks, chunk_filename, chunk_num_days,
format(chunk_start_date), format(chunk_end_date)))
# Read data block
data <- read_spatial_block(nc, varname, lonname, latname,
lon_indices, lat_indices,
time_start_global, chunk_num_days)
# data is [nlon, nlat, ntime]
# Reshape to flat array: coordinate-major, time-minor
# Order: coord[0] all days, coord[1] all days, ...
nlon <- dim(data)[1]
nlat <- dim(data)[2]
ntime <- dim(data)[3]
# Flatten to 1D: [coord_idx * ntime + day_idx]
flat_data <- numeric(num_coords * chunk_num_days)
coord_idx <- 0
for (j in 1:nlat) {
for (i in 1:nlon) {
for (t in 1:ntime) {
flat_idx <- coord_idx * ntime + t
flat_data[flat_idx] <- data[i, j, t]
}
coord_idx <- coord_idx + 1
}
}
# Calculate statistics (excluding NaN)
valid_data <- flat_data[!is.na(flat_data)]
stat_min <- if (length(valid_data) > 0) min(valid_data) else NA
stat_max <- if (length(valid_data) > 0) max(valid_data) else NA
stat_mean <- if (length(valid_data) > 0) mean(valid_data) else NA
missing_percent <- (sum(is.na(flat_data)) / length(flat_data)) * 100
# Write binary file (raw float32)
con <- file(chunk_path, "wb")
writeBin(as.numeric(flat_data), con, size = 4, endian = "little")
close(con)
file_size <- file.info(chunk_path)$size
# Store chunk info
chunk_info[[chunk_idx]] <- list(
file = file.path(attr_name, chunk_filename),
startYear = chunk_start_year,
startDayOfYear = chunk_day_of_year,
numDays = chunk_num_days,
numCoordinates = num_coords,
sizeBytes = file_size,
statMin = stat_min,
statMax = stat_max,
statMean = stat_mean,
missingDataPercent = missing_percent
)
cat(sprintf("    Stats: min=%.2f, max=%.2f, mean=%.2f, missing=%.2f%%\n",
stat_min, stat_max, stat_mean, missing_percent))
}
# Return attribute info
list(
name = attr_name,
description = attr_desc,
unit = attr_unit,
rangeMin = range_min,
rangeMax = range_max,
chunks = chunk_info,
numCoords = num_coords,
lonIndices = lon_indices,
latIndices = lat_indices,
lonFiltered = lon_filtered,
latFiltered = lat_filtered
)
}
# ============================================================================
# 4. WRITE LOOKUP TABLE FILE
# ============================================================================
write_lookup_table <- function(config, lon_filtered, lat_filtered, lon_indices, lat_indices) {
lookup_path <- file.path(config$dataFolder, "coords_lookup.bin")
cat(sprintf("\nWriting coordinate lookup table: %s\n", lookup_path))
# Calculate grid dimensions at 0.1° resolution
grid_resolution <- 0.1
num_lon_cells <- round((config$maxLongitude - config$minLongitude) / grid_resolution) + 1
num_lat_cells <- round((config$maxLatitude - config$minLatitude) / grid_resolution) + 1
total_cells <- num_lon_cells * num_lat_cells
cat(sprintf("  Grid dimensions: %d (lon) x %d (lat) = %d cells\n",
num_lon_cells, num_lat_cells, total_cells))
# Initialize lookup table with -1 (no data)
lookup_table <- rep(-1L, total_cells)
# Build coordinate map: (lon, lat) -> coordinate index
coord_idx <- 0
coord_map <- list()
for (j_idx in seq_along(lat_indices)) {
for (i_idx in seq_along(lon_indices)) {
lat <- lat_filtered[j_idx]
lon <- lon_filtered[i_idx]
# Calculate grid indices
gridX <- round((lon - config$minLongitude) / grid_resolution)
gridY <- round((lat - config$minLatitude) / grid_resolution)
# Calculate 1D lookup index
lookup_idx <- gridY * num_lon_cells + gridX + 1  # +1 for R 1-based indexing
if (lookup_idx >= 1 && lookup_idx <= total_cells) {
lookup_table[lookup_idx] <- coord_idx
}
coord_idx <- coord_idx + 1
}
}
num_valid <- sum(lookup_table >= 0)
num_invalid <- sum(lookup_table < 0)
cat(sprintf("  Valid coordinates: %d\n", num_valid))
cat(sprintf("  Invalid/ocean cells: %d\n", num_invalid))
# Write binary file
con <- file(lookup_path, "wb")
# Header
writeBin(charToRaw("LOOK"), con)  # magic
writeBin(as.integer(1), con, size = 4)  # version
writeBin(as.integer(num_lon_cells), con, size = 4)  # numLonCells
writeBin(as.integer(num_lat_cells), con, size = 4)  # numLatCells
writeBin(as.numeric(config$minLongitude), con, size = 4)
writeBin(as.numeric(config$maxLongitude), con, size = 4)
writeBin(as.numeric(config$minLatitude), con, size = 4)
writeBin(as.numeric(config$maxLatitude), con, size = 4)
writeBin(as.numeric(grid_resolution), con, size = 4)
writeBin(as.integer(num_valid), con, size = 4)  # numValidCoordinates
writeBin(as.integer(0), con, size = 4)  # reserved1
writeBin(as.integer(0), con, size = 4)  # reserved2
# Lookup table data (int32 array)
writeBin(as.integer(lookup_table), con, size = 4)
close(con)
cat(sprintf("  Lookup table written successfully\n"))
return(num_valid)
}
# ============================================================================
# 5. WRITE SPECIFICATION XML
# ============================================================================
write_specification_xml <- function(config, attributes_info, num_valid_coords, processing_time) {
spec_path <- file.path(config$dataFolder, "specification.xml")
cat(sprintf("\nWriting specification: %s\n", spec_path))
# Calculate grid dimensions
grid_resolution <- 0.1
num_lon_cells <- round((config$maxLongitude - config$minLongitude) / grid_resolution) + 1
num_lat_cells <- round((config$maxLatitude - config$minLatitude) / grid_resolution) + 1
num_chunks <- length(attributes_info[[1]]$chunks)
start_date <- as.Date(sprintf("%d-01-01", config$startYear))
end_date <- as.Date(sprintf("%d-12-31", config$endYear))
total_days <- as.integer(end_date - start_date + 1)
# Open XML file
con <- file(spec_path, "w")
# Write XML header and root
writeLines('<?xml version="1.0" encoding="utf-8"?>', con)
writeLines(sprintf('<WeatherDataSpecification'), con)
writeLines(sprintf('    version="1"'), con)
writeLines(sprintf('    created="%s"', format(Sys.time(), "%Y-%m-%dT%H:%M:%SZ")), con)
writeLines(sprintf('    source="E-OBS v31.0e"'), con)
writeLines(sprintf('    converter="eobs_chunk_exporter.R v1.0"'), con)
writeLines(sprintf('    numCoordinates="%d"', num_valid_coords), con)
writeLines(sprintf('    lookupTableFile="coords_lookup.bin"'), con)
writeLines(sprintf('    gridResolution="%.1f"', grid_resolution), con)
writeLines(sprintf('    numLonCells="%d"', num_lon_cells), con)
writeLines(sprintf('    numLatCells="%d"', num_lat_cells), con)
writeLines(sprintf('    coordinateOrdering="LatitudeThenLongitude"'), con)
writeLines(sprintf('    minLongitude="%.6f"', config$minLongitude), con)
writeLines(sprintf('    maxLongitude="%.6f"', config$maxLongitude), con)
writeLines(sprintf('    minLatitude="%.6f"', config$minLatitude), con)
writeLines(sprintf('    maxLatitude="%.6f"', config$maxLatitude), con)
writeLines(sprintf('    startYear="%d"', config$startYear), con)
writeLines(sprintf('    endYear="%d"', config$endYear), con)
writeLines(sprintf('    totalDays="%d"', total_days), con)
writeLines(sprintf('    chunkSizeDays="%d"', config$chunkDays), con)
writeLines(sprintf('    totalChunks="%d"', num_chunks), con)
writeLines(sprintf('    dataType="float32"'), con)
writeLines(sprintf('    missingValueEncoding="NaN"'), con)
writeLines(sprintf('    processingTimeSeconds="%.2f">', processing_time), con)
writeLines('', con)
# Write attributes
for (attr in attributes_info) {
writeLines(sprintf('  <Attribute name="%s" description="%s" unit="%s" rangeMin="%.1f" rangeMax="%.1f">',
attr$name, attr$description, attr$unit, attr$rangeMin, attr$rangeMax), con)
# Write bins
for (chunk in attr$chunks) {
writeLines(sprintf('    <Bin file="%s" startYear="%d" startDayOfYear="%d" numDays="%d" numCoordinates="%d" sizeBytes="%d" statMin="%.4f" statMax="%.4f" statMean="%.4f" missingDataPercent="%.4f" />',
chunk$file, chunk$startYear, chunk$startDayOfYear, chunk$numDays,
chunk$numCoordinates, chunk$sizeBytes, chunk$statMin, chunk$statMax,
chunk$statMean, chunk$missingDataPercent), con)
}
writeLines('  </Attribute>', con)
writeLines('', con)
}
# Processing log (placeholder)
writeLines('  <ProcessingLog>', con)
writeLines(sprintf('    <!-- Processing completed successfully in %.2f seconds -->', processing_time), con)
writeLines('  </ProcessingLog>', con)
writeLines('', con)
# Close root
writeLines('</WeatherDataSpecification>', con)
close(con)
cat("  Specification written successfully\n")
}
# ============================================================================
# 6. MAIN EXECUTION
# ============================================================================
main <- function() {
cat("============================================================\n")
cat("E-OBS Chunk Exporter\n")
cat("============================================================\n\n")
start_time <- Sys.time()
# Read configuration
config <- read_config("config.xml")
cat("Configuration:\n")
cat(sprintf("  Start Year: %d\n", config$startYear))
cat(sprintf("  End Year: %d\n", config$endYear))
cat(sprintf("  Chunk Days: %d\n", config$chunkDays))
cat(sprintf("  Longitude: %.2f to %.2f\n", config$minLongitude, config$maxLongitude))
cat(sprintf("  Latitude: %.2f to %.2f\n", config$minLatitude, config$maxLatitude))
cat(sprintf("  Data Folder: %s\n\n", config$dataFolder))
# Create attribute subdirectories (output goes in same folder as input)
# Main data folder should already exist (contains NetCDF files)
# Define attributes to process
attributes_config <- list(
list(
name = "TN",
file = "tn_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Minimum Temperature",
unit = "°C",
rangeMin = -50.0,
rangeMax = 50.0
),
list(
name = "TX",
file = "tx_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Maximum Temperature",
unit = "°C",
rangeMin = -50.0,
rangeMax = 50.0
),
list(
name = "RR",
file = "rr_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Precipitation",
unit = "mm",
rangeMin = 0.0,
rangeMax = 100.0
),
list(
name = "FG",
file = "fg_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Wind Speed",
unit = "m/s",
rangeMin = 0.0,
rangeMax = 10.0
),
list(
name = "HU",
file = "hu_ens_mean_0.1deg_reg_v31.0e.nc",
description = "Relative Humidity",
unit = "%",
rangeMin = 0.0,
rangeMax = 100.0
)
)
# Process each attribute
attributes_info <- list()
for (attr_cfg in attributes_config) {
nc_path <- file.path(config$dataFolder, attr_cfg$file)
if (!file.exists(nc_path)) {
warning(sprintf("File not found: %s - skipping", nc_path))
next
}
attr_info <- process_attribute(
nc_path = nc_path,
config = config,
attr_name = attr_cfg$name,
attr_desc = attr_cfg$description,
attr_unit = attr_cfg$unit,
range_min = attr_cfg$rangeMin,
range_max = attr_cfg$rangeMax
)
attributes_info[[attr_cfg$name]] <- attr_info
}
# Write lookup table (use first attribute's coordinate info)
first_attr <- attributes_info[[1]]
num_valid_coords <- write_lookup_table(config, first_attr$lonFiltered, first_attr$latFiltered,
first_attr$lonIndices, first_attr$latIndices)
# Calculate processing time
end_time <- Sys.time()
processing_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
# Write specification XML
write_specification_xml(config, attributes_info, num_valid_coords, processing_time)
cat("\n============================================================\n")
cat(sprintf("Processing complete! Total time: %.2f seconds\n", processing_time))
cat("============================================================\n")
}
# Run main
main()
